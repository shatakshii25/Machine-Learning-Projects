{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPqnv+spOEddyWfOJ3qTOcw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shatakshii25/Machine-Learning-Projects/blob/main/CNNonMNISTDatasetFromScratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CNN with two Conv layers**"
      ],
      "metadata": {
        "id": "UDa0SYBmT6qt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing libraries**"
      ],
      "metadata": {
        "id": "IdqXDX_zUDwy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CxQlr1fgTfRz"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np \n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import keras"
      ],
      "metadata": {
        "id": "AuTxTRq-UJLx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras import callbacks\n",
        "from keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import SGD, schedules"
      ],
      "metadata": {
        "id": "0v_1Q30XUKzk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "IaiTJ5XHUMpf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading the dataset**"
      ],
      "metadata": {
        "id": "CrxDrqoNUSD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This class contains helper functions to carry out the given task. Functions such as loading the dataset, preprocessing the data, augmenting the data have been included\n",
        "class Util:\n",
        "    \n",
        "    # load train and test dataset\n",
        "    def load_dataset(self):\n",
        "        # load dataset\n",
        "        (trainX, trainY), (testX, testY) = fashion_mnist.load_data()\n",
        "        # reshape dataset to have a single channel\n",
        "        trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\n",
        "        testX = testX.reshape((testX.shape[0], 28, 28, 1))\n",
        "        # one hot encode target values\n",
        "        trainY = to_categorical(trainY)\n",
        "        testY = to_categorical(testY)\n",
        "        return trainX, trainY, testX, testY\n",
        "    \n",
        "    # scale pixels\n",
        "    def prep_pixels(self, train, test):\n",
        "        # convert from integers to floats\n",
        "        train_norm = train.astype('float32')\n",
        "        test_norm = test.astype('float32')\n",
        "        # normalize to range 0-1\n",
        "        train_norm = train_norm / 255.0\n",
        "        test_norm = test_norm / 255.0\n",
        "        # return normalized images\n",
        "        return train_norm, test_norm\n",
        "    \n",
        "    # Create an ImageDataGenerator with a given set of augmentations\n",
        "    def augment_data(self,augmentations):\n",
        "        return ImageDataGenerator(width_shift_range = augmentations['width_shift_range'],\n",
        "                                height_shift_range = augmentations['height_shift_range'],\n",
        "                                horizontal_flip = augmentations['horizontal_flip'])"
      ],
      "metadata": {
        "id": "KfNbIuwNUPCB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model**"
      ],
      "metadata": {
        "id": "TNSZ5NAHUcou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model:\n",
        "    def model1(self):  ## A1 : CNN with two Conv layers\n",
        "        model = Sequential()\n",
        "        model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n",
        "        model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
        "        model.add(Dense(10, activation='softmax'))\n",
        "        # LEARNING RATE SCHEDULER\n",
        "        lr_schedule = schedules.ExponentialDecay(initial_learning_rate=1e-2,decay_steps = 10000,decay_rate = 0.9)\n",
        "        opt = SGD(learning_rate=lr_schedule)\n",
        "        model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        \n",
        "        return model"
      ],
      "metadata": {
        "id": "ibCBasQNUaH6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Driver code**"
      ],
      "metadata": {
        "id": "RflM-82wUmHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# driver for evaluating a model with a given set of augmentations\n",
        "def trainer_model1(model, data_augmentations, msg):\n",
        "    u = Util()\n",
        "    trainX, trainY, testX, testY = u.load_dataset()\n",
        "    trainX, testX = u.prep_pixels(trainX, testX)\n",
        "    model = model()\n",
        "    datagen = u.augment_data(data_augmentations)\n",
        "    it_train = datagen.flow(trainX, trainY, batch_size=32)\n",
        "    steps = int(trainX.shape[0] / 32)\n",
        "    \n",
        "    print('\\n' + msg + '\\n')\n",
        "    history = model.fit_generator(it_train, steps_per_epoch=steps, \n",
        "                                  epochs=20, validation_data=(testX, testY), \n",
        "                                  verbose=1)\n",
        "    _, acc = model.evaluate(testX, testY, verbose=0)\n",
        "    print(\"\\nTraining complete\")\n",
        "    print(\"Accuracy : \",acc)"
      ],
      "metadata": {
        "id": "YokAN2ibUj_4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = Model()\n",
        "\n",
        "augmentation = {\n",
        "    'width_shift_range' : 0.1 ,\n",
        "    'height_shift_range' : 0.1,\n",
        "    'horizontal_flip' : True\n",
        "}"
      ],
      "metadata": {
        "id": "egOOenDhUqbh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training the models**"
      ],
      "metadata": {
        "id": "fJ16I_TmUvXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_model1(m.model1 , augmentation, 'Training Model1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmrQtggnUs2k",
        "outputId": "af53f17c-dee7-42cd-fb84-9fc9d557512f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n",
            "\n",
            "Training Model1\n",
            "\n",
            "Epoch 1/20\n",
            "1875/1875 [==============================] - 98s 52ms/step - loss: 0.7328 - accuracy: 0.7292 - val_loss: 0.5710 - val_accuracy: 0.7713\n",
            "Epoch 2/20\n",
            "1875/1875 [==============================] - 95s 51ms/step - loss: 0.5388 - accuracy: 0.7984 - val_loss: 0.4517 - val_accuracy: 0.8334\n",
            "Epoch 3/20\n",
            "1875/1875 [==============================] - 94s 50ms/step - loss: 0.4772 - accuracy: 0.8227 - val_loss: 0.4175 - val_accuracy: 0.8486\n",
            "Epoch 4/20\n",
            "1875/1875 [==============================] - 94s 50ms/step - loss: 0.4505 - accuracy: 0.8344 - val_loss: 0.3936 - val_accuracy: 0.8566\n",
            "Epoch 5/20\n",
            "1875/1875 [==============================] - 93s 50ms/step - loss: 0.4229 - accuracy: 0.8429 - val_loss: 0.3899 - val_accuracy: 0.8586\n",
            "Epoch 6/20\n",
            "1875/1875 [==============================] - 92s 49ms/step - loss: 0.4097 - accuracy: 0.8489 - val_loss: 0.3586 - val_accuracy: 0.8700\n",
            "Epoch 7/20\n",
            "1875/1875 [==============================] - 92s 49ms/step - loss: 0.3976 - accuracy: 0.8523 - val_loss: 0.3688 - val_accuracy: 0.8696\n",
            "Epoch 8/20\n",
            "1875/1875 [==============================] - 92s 49ms/step - loss: 0.3853 - accuracy: 0.8572 - val_loss: 0.3648 - val_accuracy: 0.8679\n",
            "Epoch 9/20\n",
            "1875/1875 [==============================] - 94s 50ms/step - loss: 0.3767 - accuracy: 0.8606 - val_loss: 0.3323 - val_accuracy: 0.8838\n",
            "Epoch 10/20\n",
            "1875/1875 [==============================] - 95s 50ms/step - loss: 0.3671 - accuracy: 0.8640 - val_loss: 0.3413 - val_accuracy: 0.8777\n",
            "Epoch 11/20\n",
            "1875/1875 [==============================] - 95s 51ms/step - loss: 0.3600 - accuracy: 0.8674 - val_loss: 0.3483 - val_accuracy: 0.8707\n",
            "Epoch 12/20\n",
            "1875/1875 [==============================] - 94s 50ms/step - loss: 0.3520 - accuracy: 0.8708 - val_loss: 0.3310 - val_accuracy: 0.8809\n",
            "Epoch 13/20\n",
            "1875/1875 [==============================] - 95s 51ms/step - loss: 0.3463 - accuracy: 0.8718 - val_loss: 0.3203 - val_accuracy: 0.8847\n",
            "Epoch 14/20\n",
            "1875/1875 [==============================] - 92s 49ms/step - loss: 0.3425 - accuracy: 0.8732 - val_loss: 0.3279 - val_accuracy: 0.8800\n",
            "Epoch 15/20\n",
            "1875/1875 [==============================] - 92s 49ms/step - loss: 0.3360 - accuracy: 0.8764 - val_loss: 0.3324 - val_accuracy: 0.8767\n",
            "Epoch 16/20\n",
            "1875/1875 [==============================] - 92s 49ms/step - loss: 0.3342 - accuracy: 0.8753 - val_loss: 0.3362 - val_accuracy: 0.8806\n",
            "Epoch 17/20\n",
            "1875/1875 [==============================] - 92s 49ms/step - loss: 0.3282 - accuracy: 0.8784 - val_loss: 0.3200 - val_accuracy: 0.8828\n",
            "Epoch 18/20\n",
            "1875/1875 [==============================] - 95s 51ms/step - loss: 0.3259 - accuracy: 0.8795 - val_loss: 0.3194 - val_accuracy: 0.8839\n",
            "Epoch 19/20\n",
            "1875/1875 [==============================] - 96s 51ms/step - loss: 0.3190 - accuracy: 0.8818 - val_loss: 0.3068 - val_accuracy: 0.8894\n",
            "Epoch 20/20\n",
            "1875/1875 [==============================] - 95s 51ms/step - loss: 0.3167 - accuracy: 0.8833 - val_loss: 0.3064 - val_accuracy: 0.8923\n",
            "\n",
            "Training complete\n",
            "Accuracy :  0.892300009727478\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final Accuracy for Model (CNN with two Conv layers) = 89.23%**"
      ],
      "metadata": {
        "id": "P-srwpt2U31a"
      }
    }
  ]
}